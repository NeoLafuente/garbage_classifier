{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269c0c53-7478-49c4-9e49-5f4fcb6a64bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a28e7ca7-1ae5-4f56-82c5-4d2857337432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import zipfile\n",
    "import shutil\n",
    "import random\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75464aa0-66bc-45fa-a9fb-3696e7ab4973",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_DIR = \"../data/raw/sample_dataset\"\n",
    "ZIP_FILE = \"../data/raw/garbage-dataset.zip\"\n",
    "\n",
    "# Direct Kaggle download link\n",
    "KAGGLE_URL = \"https://www.kaggle.com/api/v1/datasets/download/zlatan599/garbage-dataset-classification\"\n",
    "NUM_IMAGES = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7efafbab-e10c-4a72-bfb7-0f5c2449a16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_with_curl():\n",
    "    \"\"\"\n",
    "    Download Kaggle dataset using curl + API credentials.\n",
    "    Requires: ~/.kaggle/kaggle.json with username + key\n",
    "    \"\"\"\n",
    "    print(\"Downloading dataset with curl...\")\n",
    "\n",
    "    # Make sure ~/.kaggle exists and is secure\n",
    "    os.makedirs(os.path.expanduser(\"~/.kaggle\"), exist_ok=True)\n",
    "    os.chmod(os.path.expanduser(\"~/.kaggle\"), 0o700)\n",
    "\n",
    "    # Run curl command (requires kaggle.json for auth)\n",
    "    cmd = f\"curl -L -o {ZIP_FILE} -u `jq -r .username ~/.kaggle/kaggle.json`:`jq -r .key ~/.kaggle/kaggle.json` {KAGGLE_URL}\"\n",
    "    os.system(cmd)\n",
    "\n",
    "    print(\"Extracting dataset...\")\n",
    "    with zipfile.ZipFile(ZIP_FILE, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(\"../data/raw/\")\n",
    "    \n",
    "    os.remove(ZIP_FILE)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4892b0ea-e20a-498c-b82c-262ae3d2df68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratify_dataset():\n",
    "    \"\"\"Keep only 20 images per class and copy them into the target directory\"\"\"\n",
    "    dataset = datasets.ImageFolder(\"../data/raw/Garbage_Dataset_Classification/images\")\n",
    "    class_to_indices = {cls: [] for cls in range(len(dataset.classes))}\n",
    "\n",
    "    for idx, (_, label) in enumerate(dataset.samples):\n",
    "        class_to_indices[label].append(idx)\n",
    "\n",
    "    os.makedirs(TARGET_DIR, exist_ok=True)\n",
    "\n",
    "    for class_idx, indices in class_to_indices.items():\n",
    "        class_name = dataset.classes[class_idx]\n",
    "        class_dir = os.path.join(TARGET_DIR, class_name)\n",
    "        os.makedirs(class_dir, exist_ok=True)\n",
    "\n",
    "        selected = random.sample(indices, min(NUM_IMAGES, len(indices)))\n",
    "\n",
    "        for idx in selected:\n",
    "            src_path, _ = dataset.samples[idx]\n",
    "            shutil.copy(src_path, class_dir)\n",
    "    \n",
    "    print(\"Reduced dataset created at:\", TARGET_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f45cd97-b3c3-4cb5-aa90-d5cfe5700c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset with curl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sh: 1: jq: not found\n",
      "sh: 1: jq: not found\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  120M  100  120M    0     0  29.0M      0  0:00:04  0:00:04 --:--:-- 38.9M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset...\n",
      "Reduced dataset created at: ../data/raw/sample_dataset\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(TARGET_DIR):\n",
    "    download_with_curl()\n",
    "    stratify_dataset()\n",
    "else:\n",
    "    print(f\"{TARGET_DIR} already exists, nothing to do.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
